---
title: "INDE498_HW1"
output: html_document
---

```{r setup, include=FALSE}
library(rpart)
library(MASS)

df.car <- read.csv("cu.summary.csv")
df.males <- read.csv("Males.csv")

#getting rid of column identifier
df.males <- df.males[,-1]
df.car <- df.car[,-1]
```

# Question 1
Repeat the analysis shown in the R lab of this chapter, but use TOTAL13 as the outcome variable. Please build both the regression model and the decision tree model (for regression). Identify the final models you would select, evaluate the models, and compare the regression model with the tree model.

```{r}
library(RCurl)
#### Example: Alzheimer's Disease
# filename
AD <- read.csv(text=getURL("https://raw.githubusercontent.com/shuailab/ind_498/master/resource/data/AD2.csv"))
AD$ID = c(1:dim(AD)[1])
str(AD)
```

```{r}
# subset of variables we want in our first model that only uses demographics predictors
AD_demo <- subset(AD, select=c("TOTAL13", "AGE","PTGENDER","PTEDUCAT","ID"))
str(AD_demo)
```

```{r}
# ggplot: Plot the scatterplot of the data
# install.packages("ggplot2")
library(ggplot2)
p <- ggplot(AD_demo, aes(x = PTEDUCAT, y = TOTAL13))
p <- p + geom_point(size=4)
# p <- p + geom_smooth(method = "auto")
p <- p + labs(title="TOTAL13 versus PTEDUCAT")
print(p)
```
Scatter plot "TOTAL13 versus PTEDUCAT" shows a weak positive relationship between predictors with TOTAL13

```{r}
# ggplot: Plot the scatterplot of the data
# install.packages("ggplot2")
library(ggplot2)
p <- ggplot(AD_demo, aes(x = AGE, y = TOTAL13))
p <- p + geom_point(size=4)
# p <- p + geom_smooth(method = "auto")
p <- p + labs(title="TOTAL13 versus AGE")
print(p)
```
Scatter plot "TOTAL13 versus AGE" shows a weak positive relationship between predictors with TOTAL13

```{r}
# install.packages("car")
library(car)
# fit a simple linear regression model with only AGE
lm.AD_demo <- lm(TOTAL13 ~ AGE, data = AD_demo)
# use summary() to get t-tests of parameters (slope, intercept)
summary(lm.AD_demo)
```
'AGE' is statistically significant with p-value of 0.000116, rejecting null hypothesis (H0: no relationship between TOTAL 13 and AGE). R-squared is 0.02845, indicating that 'AGE' predictor represents only 2.8% of variability in TOTAL 13

```{r}
# fit the multiple linear regression model with more than one predictor
lm.AD_demo2 <- lm(TOTAL13 ~ AGE + PTGENDER + PTEDUCAT, data = AD_demo)
summary(lm.AD_demo2)
```

With all three demographics varibles included into the model, R-squared value increased to 0.05359, indicating 5.4% of variability in TOTAL 13 can be explained by the three variables. P-value of all three variabbles are significant as their p-values are 0.00106, 0.00220,0.00830, all less than 0.01. 

```{r}
# How to detect interaction terms by exploratory data analysis (EDA)
require(ggplot2)
p <- ggplot(AD_demo, aes(x = PTEDUCAT, y = TOTAL13))
p <- p + geom_point(aes(colour=AGE), size=2)
# p <- p + geom_smooth(method = "auto")
p <- p + labs(title="TOTAL13 versus PTEDUCAT")
print(p)
```

Because the relationship between Total 13 and PTEDUCAT changes according to the levels of AGE, the same scatterplot on two levels of age can be examined.

```{r}
p <- ggplot(AD_demo[which(AD_demo$AGE < 60),], aes(x = PTEDUCAT, y = TOTAL13))
p <- p + geom_point(size=2)
p <- p + geom_smooth(method = lm)
p <- p + labs(title="TOTAL13 versus PTEDUCAT when AGE < 60")
print(p)

p <- ggplot(AD_demo[which(AD_demo$AGE > 80),], aes(x = PTEDUCAT, y = TOTAL13))
p <- p + geom_point(size=2)
p <- p + geom_smooth(method = lm)
p <- p + labs(title="TOTAL13 versus PTEDUCAT when AGE > 80")
print(p)
```

TOTAL13 shows very weak positive correlation with PTEDUCAT when AGE < 60. On the other hand, TOTAL13 is negatively correlated with PTEDUCAT for those who are older than 80 years old.



```{r}

p <- ggplot(AD_demo, aes(x = AGE, y = TOTAL13))
p <- p + geom_point(size=2)
# p <- p + geom_smooth(method = "auto")
p <- p + labs(title="TOTAL13 versus Age")
print(p)

# re-draw the scatterplot for male
p <- ggplot(AD_demo[which(AD_demo$PTGENDER == 1),], aes(x = AGE, y = TOTAL13))
p <- p + geom_point(size=2)
# p <- p + geom_smooth(method = "auto")
p <- p + labs(title="TOTAL13 versus Age - male")
print(p)


# re-draw the scatterplot for female
p <- ggplot(AD_demo[which(AD_demo$PTGENDER == 2),], aes(x = AGE, y = TOTAL13))
p <- p + geom_point(size=2)
# p <- p + geom_smooth(method = "auto")
p <- p + labs(title="TOTAL13 versus Age - female")
print(p)
```
Scatter plot between AGE and TOTAL13 for male shows more spread out patterns than those for female.

```{r}
# fit the multiple linear regression model with an interaction term: AGE*PTEDUCAT
lm.AD_demo2 <- lm(TOTAL13 ~ AGE + PTGENDER + PTEDUCAT + AGE*PTEDUCAT, data = AD_demo)
summary(lm.AD_demo2)
```
Including the interaction term increased R-squared value from 0.05359 (from the AD_demo without the interation term) to 0.05614. However, the interaction term is not statistically significant to reject the null hypothesis because the p-value was 0.24, more than 0.05.

```{r}
# Conduct diagnostics of the model
# install.packages("ggfortify")
require("ggfortify")
autoplot(lm.AD_demo2, which = 1:6, ncol = 3, label.size = 3)
```

Residuals vs fitted values is used to detect non-linearity, unequal error variances and outliers. Our graph shows no significant patterns, indicating the model is fairly fit. The residuals is clustered around the fitted line. Scale-Location and standarized resial shows no significant patterns either.

Normal Q-Q shows if the data came from some theoretical distribution (ex. normal or exponential). Our graph is not perfectly straight, showing the opportunity to improve the model. 

We tried add more predictors to improve the model:

```{r}
# fit a full-scale model
AD_full <- AD[,c(1:17)]
lm.AD <- lm(TOTAL13 ~ ., data = AD_full)
# anova(lm.AD, type=3)
summary(lm.AD)
```

To predict TOTAL13 VALUE, a full model was built with all the demographics,genetics and imaging variables. FDG, AV45, Hipponv, MMSCORE are significant based on p-values. 
R-squared is now increased to 0.5673, indicating 56% of the variability in TOTAL13 can be explained by the variables.

```{r}
# Do we need all the variables?
# remove age, as it is least significant with p-value of 0.702677
lm.AD.reduced <- lm.AD;
lm.AD.reduced <- update(lm.AD.reduced, ~ . - AGE); 
summary(lm.AD.reduced);
```
R-squared value was slightly reduced from 0.5673 to 0.5672, but almost no effects. We can use F-test to compare the full model with this new model by applying anova() function.

```{r}
anova(lm.AD.reduced,lm.AD)
```

By F-test, p-velue is 0.7027 which indicates that two models are statisticaly indistinguishable. 
We tried to remove the latest last significant predictor, e2_1.

```{r}
#further remove rs11136000, as it is now the least significant
lm.AD.reduced <- update(lm.AD.reduced, ~ . - e2_1); 
summary(lm.AD.reduced);
anova(lm.AD.reduced,lm.AD)
```
We can repeat this process until no more variable could be deleted or use step() function to achieve the automation of this.

```{r}
# Automatic model selection
lm.AD.F <- step(lm.AD, direction="backward", test="F")
summary(lm.AD.F)
anova(lm.AD.F ,lm.AD)
```
By using final 6 predictors, R-squared value is 0.5633, which is not too far off from the R-squared value of 0.5673 that was resulted by the model using all 16 predictors. 
By applying F-test using anova() function, two models are statistically indistingushiable with p-value as 0.9158.



```{r}

# Conduct diagnostics of the model
# install.packages("ggfortify")
library("ggfortify")
autoplot(lm.AD.F, which = 1:6, ncol = 3, label.size = 3)
```
Residuals vs fitted graph and scale location graph still do not show significant patterns in the graphs, indicating the assumption of non-linearity was not violated significantly.Normal Q-Q plot for the new model improved a lot from the previous model with only demographic predictors. 

#FINAL MODEL
Therefore, we decided to select the model with the 6 predictors (FDG,AV45,HippoNV,rs744373,rs3764650,MMSCORE) as the final model. 

```{r}
# Evaluate the variable importance by all subsets regression
# install.packages("leaps")
library(leaps)
leaps<-regsubsets(TOTAL13 ~ ., data = AD_full,nbest=4)
# view results 
summary(leaps)
# plot a table of models showing variables in each model.
# models are ordered by the selection statistic.
plot(leaps,scale="r2")
```
leaps() function shows which model acheive highest R-squared value and which variables frequently appear on these models. By observing the graph, the highest R-squared value is resulted in the model that uses 8 predictors that are PTGENDER, FDG, AV45, HippoNV, rs744373, rs610932, rs3764650, MMSCORE. 6 predictors out of these 8 predictors are found in the model that we chose as the final model.


#----limitations of linear regression-------
Passing significance test and fitting the model only mean that there is nothing significant against the model, meaning this is not the causal model therefore, other models can also fit the data possibliy. 

assumptions of the estimations
1. the estimations of the regression parameters are independent ( correlations are zero)
2. the variances of the regression parameters are the same

#----interaction terms ------
Interactions of the predictors could contribute extra prediction power. Both contextual knowledge and meticulous craftwork of anlytics are needed to recognize important interation terms to be included in the model.

Scatter plots can be used to visualize the relationship between any variable with the outcome. They provides insights on how the relationship changes according to anotherariable 

For continuous predictors:
```{r}
# Supplement the model with some visualization of the statistical patterns
# Scatterplot matrix to visualize the relationship between outcome variable with continuous predictors
library(ggplot2)
# install.packages("GGally")
library(GGally)
# draw the scatterplots and also empirical shapes of the distributions of the variables
p <- ggpairs(AD[,c(17,1,3,4,5,6)], upper = list(continuous = "points")
             , lower = list(continuous = "cor")
)
print(p)
```

For categorical predictors:
```{r}
# Boxplot to visualize the relationship between outcome variable with categorical predictors
library(ggplot2)
qplot(factor(PTGENDER), TOTAL13, data = AD, 
      geom=c("boxplot"), fill = factor(PTGENDER))
qplot(factor(rs3818361), TOTAL13, data = AD, 
      geom=c("boxplot"), fill = factor(rs3818361))
qplot(factor(rs11136000), TOTAL13, data = AD, 
      geom=c("boxplot"), fill = factor(rs11136000))
qplot(factor(rs744373), TOTAL13, data = AD, 
      geom=c("boxplot"), fill = factor(rs744373))
qplot(factor(rs610932), TOTAL13, data = AD, 
      geom=c("boxplot"), fill = factor(rs610932))
qplot(factor(rs3865444), TOTAL13, data = AD, 
      geom=c("boxplot"), fill = factor(rs3865444))


# Histogram to visualize the relationship between outcome variable with categorical predictors
library(ggplot2)
qplot(TOTAL13, data = AD, geom = "histogram",
      fill = factor(PTGENDER))
qplot(TOTAL13, data = AD, geom = "histogram",
      fill = factor(rs3818361))
qplot(TOTAL13, data = AD, geom = "histogram",
      fill = factor(rs11136000))
qplot(TOTAL13, data = AD, geom = "histogram",
      fill = factor(rs744373))
qplot(TOTAL13, data = AD[,c(10,12,15,17)], geom = "histogram",
      fill = factor(rs610932))
qplot(TOTAL13, data = AD[,c(10,12,15,17)], geom = "histogram",
      fill = factor(rs3865444))
```

#Decision tree

Using all variables:
```{r}
library(rpart)
library(rpart.plot)
library(dplyr)
library(tidyr)
library(ggplot2)
library(partykit)


library(RCurl)
#### Example: Alzheimer's Disease
# filename
AD <- read.csv(text=getURL("https://raw.githubusercontent.com/shuailab/ind_498/master/resource/data/AD2.csv"))
AD$ID = c(1:dim(AD)[1])
str(AD)

theme_set(theme_gray(base_size = 15))
data <- AD

tree <- rpart(TOTAL13 ~ ., data)
prp(tree, nn.cex = 1)


```
Because TOTAL13 is the continuous variables, the values of a variable are firstly ordered, and then the average of each pair of consecutive values is used as the splitting value. 

```{r}
print(tree$variable.importance)
```
MMSCORE  has the largest importance scores amongall variables.


The tree can be further pruned with the prune function whereas the parameter cp controls the model complexity. cp is the minimum relative error improved by splitting the node. A larger cp leads to a less-complex tree. First, let we try cp = 0.05 and then cp = 0.1
```{r}
tree_0.05 <- prune(tree, cp = 0.05)
prp(tree_0.05, nn.cex = 1)
```
```{r}
tree_0.1 <- prune(tree, cp = 0.1)
prp(tree_0.1, nn.cex = 1)
```
cp can be used to control the model complexity in pruning. cp can be decided by minimizing the cross-validation error. #For each tree, the number of decision nodes is recorded and used for measuring the complexity of the tree.

#Comparison between regression model and tree model

Decision tree selected the 4 variables : MMSCORE, HippoNV, FDG and e4_1 as nodes. #Three variables including FDB, HippoNV, MMSCORE are the common variables with the fianl model with 6 valueables (FDG,AV45,HippoNV,rs744373,rs3764650,MMSCORE).

when applying a decision tree to a dataset with linear relationship between predictors and outcome variables, it may not be an optimal choice. It can be seen the classification boundary from the logistics regression model is linear, while the one from the decision tree is parallel to the axis. This limitation makes a decision tree not be able to fully capture the linear relationship in the data.

#Reference: 
textbook, PennState Eberly College of Science (https://onlinecourses.science.psu.edu/stat501), University of Virginia Library (http://data.library.virginia.edu/understanding-q-q-plots/)

#Question 2
Find two data sets from the UCI data repository or R datasets. Conduct a detailed regression analysis for both datasets using both regression model and the tree model (for regression), e.g., for regression model, you may want to conduct model selection, model comparison, testing of the significance of the regression parameters, evaluation of the R-squared and significance of the model. Also comment on the application of your model on the context of the dataset you have selected.

## Car Dataset
The first dataset we chose for modeling is the cu.summary dataset that contains automobile data taken from the April, 1990 issue of "Consumer Reports". The dataset contains 117 observations and 5 features. We are trying to predict the price of the car. A table of feature descriptions is provided below.

| Feature Name | Description                                                                                 |
|--------------|---------------------------------------------------------------------------------------------|
| Price        | a numeric vector giving the list price in US dollars of a standard model                    |
| Country      | Country of origin                                                                           |
| Reliability  | an ordered factor with levels 'Much worse' < 'worse' < 'average' < 'better' < 'Much better' |
| Mileage      | fuel consumption miles per US gallon                                                        |
| Type         | a factor with levels Compact Large Medium Small Sporty Van                                  |

### Linear Regression
We fit a linear regression model with all of the features to predict the price of the car. 
Looking at the summary, we see that this linear regression model explains 83.7% of the 
variability in the data. 
```{r}
lm.car <- lm(Price~., data=df.car)
summary(lm.car)
```

We use the stepAIC function to identify and discard the insignificant features in our model. 
Looking at the summary, we see that none of the features have been discarded, showing that 
all of the features in our initial linear regression are significant. Also, let us note that 
our initial model is the optimal linear regression model given the data. 

Looking at the summary, we see that the features which increase the price are: the size of the vehicle (medium or large), 
a better reliability, the country it was made in (Japan and Sweden), and the type of vehicle (sporty or van). Thus, three out of the four
factors positively affect the price. The coefficients of the other features were all negative. 

```{r}
lm.car <- stepAIC(lm.car, direction="both")
summary(lm.car)
```

We now analyze our final linear regression model by use of the residual vs fitted and QQ-plots. The residual vs fitted plot 
shows randomly distributed points, which means that the regression assumptions have not been violated. The QQ-plot shows 
points clustered along the line indicating that the dependent variables can plausibly be normally distributed. 

```{r}
par(mar = rep(2, 4)) # Change the margins
plot(lm.car, which=c(1))
plot(lm.car, which=c(2))
```

This model can be used to study which factors have the biggest impact on increasing the price of a vehicle. 
Understanding the extent in which certain features inflate the price of a vehicle can help manufacturers decide which
areas they should focus on when attempting to decrease their prices.


### Decision Tree
A model of the decision tree fitted to our dataset is produced below. The decision tree only incorporates 
two out of the four features; the tree incorporates the type and country of origin of each vehicle while 
it discards the information pertaining to the reliability and mileage of the vehicle. In contrast, 
the linear regression model incorporated all four of the given features and showed that including three of the 
features would always positively affect the price. This discrepancy between models suggests that each model 
may be able to better explain certain characteristics of the data. 

```{r}
library("rpart.plot")

tr.car <- rpart(Price~., data=df.car)
prp(tr.car, varlen=3)
```

## Males Dataset
The second dataset we chose for modeling is the Males dataset that recorded data about the wages and education of young males. The dataset contained 4360 observations and 12 features.  We are trying to predict the wage of the person. A table of feature descriptions is provided below.

| Feature Name | Description                        |
|--------------|------------------------------------|
| nr           | unique identifier                  |
| year         | year data was collected            |
| school       | years of schooling                 |
| exper        | years of experience                |
| union        | wage set by collective bargaining? |
| ethn         | ethnicity                          |
| married      | are they married?                  |
| health       | health problems?                   |
| wage         | log of hourly wage                 |
| industry     | work industry                      |
| occupation   | job occupation                     |
| residence    | area of residence                  |
### Linear Regression
We first fit an initial linear regression model with all of the features to predict the wage of the individual. From looking at the summary, the initial linear regression model is able to explain 28.54% of the variability of the wage. 

```{r}
lm.males <- lm(wage~., data=df.males)
summary(lm.males)
```
Next we used some feature selection using stepwise regresssion to fit the best model. The final model produced from the stepwise regression found all but the health feature to be signficant. By analyzing the difference in the models, we can see that the R-squared was only reduced to 28.53%. The final found every feature and the intercept to be significant. When looking at the coeffecients, that years of schooling, year of experience, union, ethnicity, and being married all had a positive impact on wages per unit increase while keeping all others constant. Other coeffecients were a mix of positive and negative values. 

```{r}
lm.males <- stepAIC(lm.males, direction="both")
summary(lm.males)
```
After obtaining this final model, we need to analyze the residual vs fitted plot and the QQ-plot to see if any of the linear regression assumptions are violated. From looking at the residual vs fitted plot, the points seem to be randomly distrubted and there are no trends as fitted values increase, so there does not seem to be any violations. When looking at the QQ-plot, there seems to be deviation from normality assumption. 

```{r}
plot(lm.males, which=c(1))
plot(lm.males, which=c(2))
```
The applications of this model could be used to study what types of features are correlated most with a high wage. This could be important in helping to decide policy of whether more education helps to increase wage and to study if there are discrepencies among different socioeconomic factors. 

### Decision Tree
A decision tree was fit to the data and the output of the model is seen below. Compared to the regression model, the decision tree only incorporated the use of 5 out of 11 features. These features are industry, years of schooling, year the data was collected, married, and years of work experience. From the analyzing the rules the decision tree came up with, the observations with the highest wage are those who have 12+ years of school. This type can be used to see if there are any underlying patterns in the data that a linear regression might not be able to ascertain. 
```{r}
tr.males <- rpart(wage~.,data=df.males)
prp(tr.males, nn.cex=1)
```

# Question 5 
Build a decision tree model based on the following dataset. Don't use R. Use your pen and paper, and show the process.

```{r}
df.q5 <- data.frame(x1=c(0.22,0.58,0.57,0.41,0.60,0.12,0.25,0.32), 
                    x2=c(0.38,0.32,0.28,0.43,0.29,0.32,0.32,0.38), 
                    y=factor(c("No","Yes","Yes","Yes","No","Yes","Yes","No")))
 
```

We chose an arbitrary rule of splitting on the 4 quantiles. The first split is on x2 at the 3rd quantile, which is $x2 \leq 0.38$. The left node will contain data points (1,2,3,5,6,7,8) and the right node will contain data point (4).

\[e_{root} = -\frac{5}{8} log_{2}\frac{5}{8} - \frac{3}{8} log_{2}\frac{3}{8} = 0.9544\]
\[e_{x2 \leq 0.38} =  -\frac{4}{7} log_{2}\frac{4}{7} - \frac{3}{7} log_{2}\frac{3}{7} = 0.9852\]
\[e_{x2 \gt 0.38}  -\frac{1}{1} log_{2}\frac{1}{1} - \frac{0}{1} log_{2}\frac{0}{1} = 0\]
\[IG = e_{root} - \frac{7}{8}*e_{x2 \leq 0.38}-\frac{1}{8}*e_{x2 \gt 0.38} = 0.0924\]

The second split was done on left node containing data points (1,2,3,5,6,7,8) on x2 at the 3rd quantile, which is $x2 \leq 0.35$. The left node will contain data points (2,3,5,6,7) and the right node will contain data points (1,8).

\[e_{root} = -\frac{4}{7} log_{2}\frac{4}{7} - \frac{3}{7} log_{2}\frac{3}{7} = 0.9852\]
\[e_{x2 \leq 0.35} =  -\frac{4}{5} log_{2}\frac{4}{5} - \frac{1}{5} log_{2}\frac{1}{5} = 0.7219\]
\[e_{x2 \gt 0.35}  -\frac{0}{2} log_{2}\frac{0}{2} - \frac{2}{2} log_{2}\frac{2}{2} = 0\]
\[IG = e_{root} - \frac{5}{7}*e_{x2 \leq 0.35}-\frac{2}{7}*e_{x2 \gt 0.35} = 0.4696\]

The third split was on the left node containin data points (2,3,5,6,7) on x1 at the 3rd quantile, which is $x1 \leq 0.58$. The left node will contain data points (2,3,6,7) and the right node will contain data point (5). 

\[e_{root} = -\frac{4}{5} log_{2}\frac{4}{5} - \frac{1}{5} log_{2}\frac{1}{5} = 0.7219\]
\[e_{x1 \leq 0.58} =  -\frac{4}{4} log_{2}\frac{4}{4} - \frac{0}{4} log_{2}\frac{0}{4} = 0\]
\[e_{x1 \gt 0.58}  -\frac{0}{1} log_{2}\frac{0}{1} - \frac{1}{1} log_{2}\frac{1}{1} = 0\]
\[IG = e_{root} - \frac{4}{5}*e_{x1 \leq 0.58}-\frac{1}{5}*e_{x1 \gt 0.58} = 0.7219\]


# Question 6
Write your own R script to implement the least squares estimation of a regression model. Compare the output from your script with the output from lm().

We arbitrarily choose the number of input variables as p = 3, and define the following function. 

```{r}
#least_squares: Returns the least squares estimation of a regression model
least_squares <- function(x1, x2, x3, y){ 
  
  # Creating the X and Y matrices
  x_matrix = as.matrix(cbind(rep(1, 20), x1, x2, x3))
  y_matrix = as.matrix(y)
  
  # Computing the beta estimator
  beta_estimator = solve( t(x_matrix) %*% x_matrix ) %*% t(x_matrix) %*% y_matrix
  
  return(beta_estimator)
} 
```

We can now compare our function against the output from lm()

```{r}
# Choose random x1, x2, x3 inputs
x1 = runif(1:20)
x2 = runif(1:20)
x3 = runif(1:20)

# Create an arbitrary y 
y = 4 + 2*x1 + 1*x2 + 8*x3

# Call our function: least_squares 
least_squares(x1, x2, x3, y)

# Calling the R function for linear regression: lm 
lm(y~., data = data.frame(x1, x2, x3, y))

```
