---
title: "INDE498_HW1"
output: html_document
---

```{r setup, include=FALSE}
library(rpart)
library(MASS)

df.car <- read.csv("cu.summary.csv")
df.males <- read.csv("Males.csv")

#getting rid of column identifier
df.males <- df.males[,-1]
df.car <- df.car[,-1]
```

# Question 1
Repeat the analysis shown in the R lab of this chapter, but use TOTAL13 as the outcome variable. Please build both the regression model and the decision tree model (for regression). Identify the final models you would select, evaluate the models, and compare the regression model with the tree model.


#Question 2
Find two data sets from the UCI data repository or R datasets. Conduct a detailed regression analysis for both datasets using both regression model and the tree model (for regression), e.g., for regression model, you may want to conduct model selection, model comparison, testing of the significance of the regression parameters, evaluation of the R-squared and significance of the model. Also comment on the application of your model on the context of the dataset you have selected.

## Car Dataset
The first dataset we chose for modeling is the cu.summary dataset that contains automobile data taken from the April, 1990 issue of "Consumer Reports". The dataset contains 117 observations and 5 features. We are trying to predict the price of the car. A table of feature descriptions is provided below.

| Feature Name | Description                                                                                 |
|--------------|---------------------------------------------------------------------------------------------|
| Price        | a numeric vector giving the list price in US dollars of a standard model                    |
| Country      | Country of origin                                                                           |
| Reliability  | an ordered factor with levels 'Much worse' < 'worse' < 'average' < 'better' < 'Much better' |
| Mileage      | fuel consumption miles per US gallon                                                        |
| Type         | a factor with levels Compact Large Medium Small Sporty Van                                  |

### Linear Regression
We first fit an initial model and analyze the summary to get an
```{r}
lm.car <- lm(Price~., data=df.car)
summary(lm.car)
lm.car <- stepAIC(lm.car, direction="both")
summary(lm.car)
```
### Decision Tree
```{r}
tr.car <- rpart(Price~., data=df.car)
prp(tr.car, nn.cex=1)
```

## Males Dataset
The second dataset we chose for modeling is the Males dataset that recorded data about the wages and education of young males. The dataset contained 4360 observations and 12 features.  We are trying to predict the wage of the person. A table of feature descriptions is provided below.

| Feature Name | Description                        |
|--------------|------------------------------------|
| nr           | unique identifier                  |
| year         | year data was collected            |
| school       | years of schooling                 |
| exper        | years of experience                |
| union        | wage set by collective bargaining? |
| ethn         | ethnicity                          |
| married      | are they married?                  |
| health       | health problems?                   |
| wage         | log of hourly wage                 |
| industry     | work industry                      |
| occupation   | job occupation                     |
| residence    | area of residence                  |
### Linear Regression
We first fit an initial linear regression model with all of the features to predict the wage of the individual. From looking at the summary, the initial linear regression model is able to explain 28.54% of the variability of the wage. 

```{r}
lm.males <- lm(wage~., data=df.males)
summary(lm.males)
```
Next we used some feature selection using stepwise regresssion to fit the best model. The final model produced from the stepwise regression found all but the health feature to be signficant. By analyzing the difference in the models, we can see that the R-squared was only reduced to 28.53%. The final found every feature and the intercept to be significant. When looking at the coeffecients, that years of schooling, year of experience, union, ethnicity, and being married all had a positive impact on wages per unit increase while keeping all others constant. Other coeffecients were a mix of positive and negative values. 

```{r}
lm.males <- stepAIC(lm.males, direction="both")
summary(lm.males)
```
After obtaining this final model, we need to analyze the residual vs fitted plot and the QQ-plot to see if any of the linear regression assumptions are violated. From looking at the residual vs fitted plot, the points seem to be randomly distrubted and there are no trends as fitted values increase, so there does not seem to be any violations. When looking at the QQ-plot, there seems to be deviation from normality assumption. 

```{r}
plot(lm.males, which=c(1))
plot(lm.males, which=c(2))
```
The applications of this model could be used to study what types of features are correlated most with a high wage. This could be important in helping to decide policy of whether more education helps to increase wage and to study if there are discrepencies among different socioeconomic factors. 

### Decision Tree
A decision tree was fit to the data and the output of the model is seen below. Compared to the regression model, the decision tree only incorporated the use of 5 out of 11 features. These features are industry, years of schooling, year the data was collected, married, and years of work experience. From the analyzing the rules the decision tree came up with, the observations with the highest wage are those who have 12+ years of school. This type can be used to see if there are any underlying patterns in the data that a linear regression might not be able to ascertain. 
```{r}
tr.males <- rpart(wage~.,data=df.males)
prp(tr.males, nn.cex=1)
```

# Question 5 
Build a decision tree model based on the following dataset. Don't use R. Use your pen and paper, and show the process.

```{r}
df.q5 <- data.frame(x1=c(0.22,0.58,0.57,0.41,0.60,0.12,0.25,0.32), 
                    x2=c(0.38,0.32,0.28,0.43,0.29,0.32,0.32,0.38), 
                    y=factor(c("No","Yes","Yes","Yes","No","Yes","Yes","No")))
 
```

We chose an arbitrary rule of splitting on the 4 quantiles. The first split is on x2 at the 3rd quantile, which is $x2 \leq 0.38$. The left node will contain data points (1,2,3,5,6,7,8) and the right node will contain data point (4).

\[e_{root} = -\frac{5}{8} log_{2}\frac{5}{8} - \frac{3}{8} log_{2}\frac{3}{8} = 0.9544\]
\[e_{x2 \leq 0.38} =  -\frac{4}{7} log_{2}\frac{4}{7} - \frac{3}{7} log_{2}\frac{3}{7} = 0.9852\]
\[e_{x2 \gt 0.38}  -\frac{1}{1} log_{2}\frac{1}{1} - \frac{0}{1} log_{2}\frac{0}{1} = 0\]
\[IG = e_{root} - \frac{7}{8}*e_{x2 \leq 0.38}-\frac{1}{8}*e_{x2 \gt 0.38} = 0.0924\]

The second split was done on left node containing data points (1,2,3,5,6,7,8) on x2 at the 3rd quantile, which is $x2 \leq 0.35$. The left node will contain data points (2,3,5,6,7) and the right node will contain data points (1,8).

\[e_{root} = -\frac{4}{7} log_{2}\frac{4}{7} - \frac{3}{7} log_{2}\frac{3}{7} = 0.9852\]
\[e_{x2 \leq 0.35} =  -\frac{4}{5} log_{2}\frac{4}{5} - \frac{1}{5} log_{2}\frac{1}{5} = 0.7219\]
\[e_{x2 \gt 0.35}  -\frac{0}{2} log_{2}\frac{0}{2} - \frac{2}{2} log_{2}\frac{2}{2} = 0\]
\[IG = e_{root} - \frac{5}{7}*e_{x2 \leq 0.35}-\frac{2}{7}*e_{x2 \gt 0.35} = 0.4696\]

The third split was on the left node containin data points (2,3,5,6,7) on x1 at the 3rd quantile, which is $x1 \leq 0.58$. The left node will contain data points (2,3,6,7) and the right node will contain data point (5). 

\[e_{root} = -\frac{4}{5} log_{2}\frac{4}{5} - \frac{1}{5} log_{2}\frac{1}{5} = 0.7219\]
\[e_{x1 \leq 0.58} =  -\frac{4}{4} log_{2}\frac{4}{4} - \frac{0}{4} log_{2}\frac{0}{4} = 0\]
\[e_{x1 \gt 0.58}  -\frac{0}{1} log_{2}\frac{0}{1} - \frac{1}{1} log_{2}\frac{1}{1} = 0\]
\[IG = e_{root} - \frac{4}{5}*e_{x1 \leq 0.58}-\frac{1}{5}*e_{x1 \gt 0.58} = 0.7219\]


# Question 6
Write your own R script to implement the least squares estimation of a regression model. Compare the output from your script with the output from lm().

We arbitrarily choose the number of input variables as p = 3, and define the following function. 

```{r}
#least_squares: Returns the least squares estimation of a regression model
least_squares <- function(x1, x2, x3, y){ 
  
  # Creating the X and Y matrices
  x_matrix = as.matrix(cbind(rep(1, 20), x1, x2, x3))
  y_matrix = as.matrix(y)
  
  # Computing the beta estimator
  beta_estimator = solve( t(x_matrix) %*% x_matrix ) %*% t(x_matrix) %*% y_matrix
  
  return(beta_estimator)
} 
```

We can now compare our function against the output from lm()

```{r}
# Choose random x1, x2, x3 inputs
x1 = runif(1:20)
x2 = runif(1:20)
x3 = runif(1:20)

# Create an arbitrary y 
y = 4 + 2*x1 + 1*x2 + 8*x3

# Call our function: least_squares 
least_squares(x1, x2, x3, y)

# Calling the R function for linear regression: lm 
lm(y~., data = data.frame(x1, x2, x3, y))

```
