---
title: "INDE498_HW1"
author: "Steven Hwang, 	Haena Kim, Victoria Diaz"
output:
  html_document: default
---

```{r setup, include=FALSE}
library(rpart)
library(MASS)

df.car <- read.csv("cu.summary.csv")
df.males <- read.csv("Males.csv")

#getting rid of column identifier
df.males <- df.males[,-1]
df.car <- df.car[,-1]
```

# Question 1
Repeat the analysis shown in the R lab of this chapter, but use TOTAL13 as the outcome variable. Please build both the regression model and the decision tree model (for regression). Identify the final models you would select, evaluate the models, and compare the regression model with the tree model.

```{r}
library(RCurl)
AD <- read.csv(text=getURL("https://raw.githubusercontent.com/shuailab/ind_498/master/resource/data/AD2.csv"))
AD$ID = c(1:dim(AD)[1])
str(AD)
```

```{r}
AD_demo <- subset(AD, select=c("TOTAL13", "AGE","PTGENDER","PTEDUCAT","ID"))
str(AD_demo)
```

```{r}

library(ggplot2)
p <- ggplot(AD_demo, aes(x = PTEDUCAT, y = TOTAL13))
p <- p + geom_point(size=4)
p <- p + labs(title="TOTAL13 versus PTEDUCAT")
print(p)
```

Scatter plot "TOTAL13 versus PTEDUCAT" shows a weak positive relationship between predictors with TOTAL13

```{r}

library(ggplot2)
p <- ggplot(AD_demo, aes(x = AGE, y = TOTAL13))
p <- p + geom_point(size=4)
p <- p + labs(title="TOTAL13 versus AGE")
print(p)
```

Scatter plot "TOTAL13 versus AGE" shows a weak positive relationship between predictors with TOTAL13

```{r}
# fit a simple linear regression model with AGE
library(car)
lm.AD_demo <- lm(TOTAL13 ~ AGE, data = AD_demo)
summary(lm.AD_demo)
```
'AGE' is statistically significant with p-value of 0.000116, rejecting null hypothesis (H0: no relationship between TOTAL 13 and AGE). R-squared is 0.02845, indicating that 'AGE' predictor represents only 2.8% of variability in TOTAL 13

```{r}
lm.AD_demo2 <- lm(TOTAL13 ~ AGE + PTGENDER + PTEDUCAT, data = AD_demo)
summary(lm.AD_demo2)
```

With all three demographics varibles included into the model, R-squared value increased to 0.05359, indicating 5.4% of variability in TOTAL 13 can be explained by the three variables. P-value of all three variabbles are significant as their p-values are 0.00106, 0.00220,0.00830, all less than 0.01. 

```{r}

require(ggplot2)
p <- ggplot(AD_demo, aes(x = PTEDUCAT, y = TOTAL13))
p <- p + geom_point(aes(colour=AGE), size=2)
p <- p + labs(title="TOTAL13 versus PTEDUCAT")
print(p)
```

Because the relationship between Total 13 and PTEDUCAT changes according to the levels of AGE, the same scatterplot on two levels of age can be examined.

```{r}
p <- ggplot(AD_demo[which(AD_demo$AGE < 60),], aes(x = PTEDUCAT, y = TOTAL13))
p <- p + geom_point(size=2)
p <- p + geom_smooth(method = lm)
p <- p + labs(title="TOTAL13 versus PTEDUCAT when AGE < 60")
print(p)

p <- ggplot(AD_demo[which(AD_demo$AGE > 80),], aes(x = PTEDUCAT, y = TOTAL13))
p <- p + geom_point(size=2)
p <- p + geom_smooth(method = lm)
p <- p + labs(title="TOTAL13 versus PTEDUCAT when AGE > 80")
print(p)
```

TOTAL13 shows very weak positive correlation with PTEDUCAT when AGE < 60. On the other hand, TOTAL13 is negatively correlated with PTEDUCAT for those who are older than 80 years old.


```{r}

p <- ggplot(AD_demo, aes(x = AGE, y = TOTAL13))
p <- p + geom_point(size=2)
p <- p + labs(title="TOTAL13 versus Age")
print(p)

#for male
p <- ggplot(AD_demo[which(AD_demo$PTGENDER == 1),], aes(x = AGE, y = TOTAL13))
p <- p + geom_point(size=2)
p <- p + labs(title="TOTAL13 versus Age - male")
print(p)


# for female
p <- ggplot(AD_demo[which(AD_demo$PTGENDER == 2),], aes(x = AGE, y = TOTAL13))
p <- p + geom_point(size=2)
p <- p + labs(title="TOTAL13 versus Age - female")
print(p)
```

Scatter plot between AGE and TOTAL13 for male shows more spread out patterns than those for female.

```{r}
# inlcuding interaction term: AGE*PTEDUCAT
lm.AD_demo2 <- lm(TOTAL13 ~ AGE + PTGENDER + PTEDUCAT + AGE*PTEDUCAT, data = AD_demo)
summary(lm.AD_demo2)
```
Including the interaction term increased R-squared value from 0.05359 (from the AD_demo without the interation term) to 0.05614. However, the interaction term is not statistically significant to reject the null hypothesis because the p-value was 0.24, more than 0.05.

```{r}
# Diagnostics graphs:
require("ggfortify")
autoplot(lm.AD_demo2, which = 1:6, ncol = 3, label.size = 3)
```

Residuals vs fitted values is used to detect non-linearity, unequal error variances and outliers. Our graph shows no significant patterns, indicating the model is fairly fit. The residuals is clustered around the fitted line. Scale-Location and standarized resial shows no significant patterns either.

Normal Q-Q shows if the data came from some theoretical distribution (ex. normal or exponential). Our graph is not perfectly straight, showing the opportunity to improve the model. 

We tried add more predictors to improve the model:

```{r}
# try full-scale model - exclude MMSCORE as it is other output 
AD_full <- AD[,c(1:17)]
AD_full <- subset(AD_full, select = -c(MMSCORE) )
names(AD_full)
lm.AD <- lm(TOTAL13 ~ ., data = AD_full)
summary(lm.AD)
```

To predict TOTAL13 Value, a full model was built with all the demographics,genetics and imaging variables.PTEDUCAT, FDG, AV45, Hipponv, rs610932 are significant based on p-values. 
R-squared is now increased to 0.4423, indicating 44% of the variability in TOTAL13 can be explained by the variables.

```{r}
# try taking AGE out to find differences
lm.AD.reduced <- lm.AD;
lm.AD.reduced <- update(lm.AD.reduced, ~ . - AGE); 
summary(lm.AD.reduced);
```
R-squared value was slightly reduced from 0.4423 to 0.4414, but almost no effects. We can use F-test to compare the full model with this new model by applying anova() function.

```{r}
anova(lm.AD.reduced,lm.AD)
```

By F-test, p-velue is 0.7692 which indicates that two models are statisticaly indistinguishable. 
We tried to remove the latest last significant predictor, e4_1.

```{r}
lm.AD.reduced <- update(lm.AD.reduced, ~ . - e4_1); 
summary(lm.AD.reduced);
anova(lm.AD.reduced,lm.AD)
```
We can repeat this process until no more variable could be deleted or use step() function to achieve the automation of this.

```{r}
# model selection
lm.AD.F <- step(lm.AD, direction="backward", test="F")
summary(lm.AD.F)
anova(lm.AD.F ,lm.AD)
```
By using final 6 predictors, R-squared value is 0.4366, which is not too far off from the R-squared value of 0.4423 that was resulted by the model using all predictors. 
By applying F-test using anova() function, two models are statistically indistingushiable with p-value as 0.8287.


```{r}
# Diagnostics graphs:
library("ggfortify")
autoplot(lm.AD.F, which = 1:6, ncol = 3, label.size = 3)
```

Residuals vs fitted graph and scale location graph still do not show significant patterns in the graphs, indicating the assumption of non-linearity was not violated significantly.Normal Q-Q plot for the new model improved a lot from the previous model with only demographic predictors. 

#FINAL MODEL
Therefore, we decided to select the model with the 6 predictors (PTEDUCAT,FDG,AV45,HippoNV,rs610932 ,rs3865444) as the final model. 


```{r}
# Evaluate the variable importance by all subsets regression
# install.packages("leaps")
library(leaps)
leaps<-regsubsets(TOTAL13 ~ ., data = AD_full,nbest=4)
# view results 
summary(leaps)
# plot a table of models showing variables in each model.
# models are ordered by the selection statistic.
plot(leaps,scale="r2")
```

leaps() function shows which model acheive highest R-squared value and which variables frequently appear on these models. By observing the graph, the highest R-squared value is resulted in the model that uses 8 predictors that are PTGENDER, PTEDUCAT, FDG, AV45, HippoNV, rs3818361, rs610932, rs3865444. All of 6 predictors used in our final model (PTEDUCAT,FDG,AV45,HippoNV,rs610932 ,rs3865444) were included in these 8 predictors.


#----limitations of linear regression-------
Passing significance test and fitting the model only mean that there is nothing significant against the model, meaning this is not the causal model therefore, other models can also fit the data possibliy. 

assumptions of the estimations
1. the estimations of the regression parameters are independent (correlations are zero)
2. the variances of the regression parameters are the same

#----interaction terms ------
Interactions of the predictors could provide useful insights additionally. However, choosing which interaction terms are difficult. Careful and thorough analytics require in selecting interaction terms before including them into the model. 

Scatter plots helps visualizing the relationship between any variable with the outcome. Insights on how the relationship changes according to another variables can be achieved. 

For continuous predictors:
```{r}
library(ggplot2)
library(GGally)
p <- ggpairs(AD[,c(17,1,3,4,5,6)], upper = list(continuous = "points")
             , lower = list(continuous = "cor")
)
print(p)
```

For categorical predictors:
```{r}
# Boxplot
library(ggplot2)
qplot(factor(PTGENDER), TOTAL13, data = AD, 
      geom=c("boxplot"), fill = factor(PTGENDER))
qplot(factor(rs3818361), TOTAL13, data = AD, 
      geom=c("boxplot"), fill = factor(rs3818361))
qplot(factor(rs11136000), TOTAL13, data = AD, 
      geom=c("boxplot"), fill = factor(rs11136000))
qplot(factor(rs744373), TOTAL13, data = AD, 
      geom=c("boxplot"), fill = factor(rs744373))
qplot(factor(rs610932), TOTAL13, data = AD, 
      geom=c("boxplot"), fill = factor(rs610932))
qplot(factor(rs3865444), TOTAL13, data = AD, 
      geom=c("boxplot"), fill = factor(rs3865444))


# Histogram
library(ggplot2)
qplot(TOTAL13, data = AD, geom = "histogram",
      fill = factor(PTGENDER))
qplot(TOTAL13, data = AD, geom = "histogram",
      fill = factor(rs3818361))
qplot(TOTAL13, data = AD, geom = "histogram",
      fill = factor(rs11136000))
qplot(TOTAL13, data = AD, geom = "histogram",
      fill = factor(rs744373))
qplot(TOTAL13, data = AD[,c(10,12,15,17)], geom = "histogram",
      fill = factor(rs610932))
qplot(TOTAL13, data = AD[,c(10,12,15,17)], geom = "histogram",
      fill = factor(rs3865444))
```

#Decision tree

Using all variables:
```{r}
library(rpart)
library(rpart.plot)
library(dplyr)
library(tidyr)
library(ggplot2)
library(partykit)


theme_set(theme_gray(base_size = 15))
data <- AD_full

tree <- rpart(TOTAL13 ~ ., data, method="anova")
prp(tree, nn.cex = 1)


```

Because TOTAL13 is the continuous variables, the average of each pair of consecutive values is used as splitting value the values.

```{r}
print(tree$variable.importance)
```
FDG  has the largest importance scores among all variables.


While cp controls the model complexity, the tree can be pruned with the prune function to minimize relative error by splitting the node. less-complex tree can be created by a larger cp. cp = 0.05 and then cp = 0.2 were used to compare the complexity of the decision tree model.
```{r}
tree_0.05 <- prune(tree, cp = 0.05)
prp(tree_0.05, nn.cex = 1)
```
```{r}
tree_0.1 <- prune(tree, cp = 0.2)
prp(tree_0.1, nn.cex = 1)
```

As cp value becomes larger, the size of decision tree becomes smaller. 



#Finding the adequate number of the leaf nodes

In order to find the adquate size of the nodes, we conducted further analysis on the model. We used the half of the data points for training a decision tree and the rest of them for testing. As training error and testing error can be calculated for each structure, the number of leaf nodes can be found and used for complexity measurement of the tree.

```{r,cache=FALSE}
set.seed(1)
train_sample <- sample(nrow(data),floor( nrow(data)/2) )
errintrain <- NULL
errintest <- NULL
leaf.v <- NULL

for(i in seq(0.2,0,by=-0.005) ){
  tree <- rpart( TOTAL13 ~ ., data = data[train_sample,], cp= i  ) 
  pred.train <- floor(predict(tree, data[train_sample,]))
  pred.test <- floor(predict(tree, data[-train_sample,]))
  current_error_train <- length(which(pred.train != data[train_sample,]$TOTAL13))/length(pred.train)
  current_error_test <- length(which(pred.test != data[-train_sample,]$TOTAL13))/length(pred.test)
  errintrain <- c(errintrain, current_error_train)
  errintest <- c(errintest, current_error_test)
  leaf.v <- c(leaf.v, length(which(tree$frame$var == "<leaf>")))
}
err.mat <- as.data.frame( cbind( train_err = errintrain, test_err = errintest , leaf_num = leaf.v ) )
err.mat$leaf_num <- as.factor( err.mat$leaf_num  )
err.mat <- unique(err.mat)
err.mat <- err.mat %>% gather(type, error, train_err,test_err)
print(err.mat)
```


The test errors and train errors according to different size of the leaf nodes are plotted.  Train errors generatlly decrease while the test errerors first decrease and then increase at leaf number eqauals to 5. Therefore, The adequate number of leaf node would be 5. Other leaf numbers may result overfitting of predicted data.
```{r}
data.plot <- err.mat %>% mutate(type = type)
ggplot(data.plot, aes(x=leaf_num, y=error, shape = type, color=type)) + geom_line() +
  geom_point(size=5) 
```

Final decision tree model can be selected with the 5 decision (leaf) nodes WITH cp = 0.02

```{r}
tree_0.020 <- prune(tree, cp = 0.02)
prp(tree_0.05, nn.cex = 1)

```


#Comparison between regression model and tree model

Decision tree used the 4 variables :  FDG, HippoNV, AV45, AGE for the 5 decision nodes (using FDG twice with different threasholds). Three variables including FDB, HippoNV, AV45 are the commonly shared with the fianl regression model with 6 valueables (PTEDUCAT,FDG,AV45,HippoNV,rs610932 ,rs3865444).

Mean square error (MSE) can be used for comparing the two models. 

MSE of our regression model can be calculated as:

```{r}
# install.packages("Metrics")
library(Metrics)

predictedvalue <- predict(lm.AD.F,data)
mse(data$TOTAL13,predictedvalue)

#double check the function mse() is working
mean((data$TOTAL13-predictedvalue)^2)


```

MSE of our decision tree model can be calculated as:

```{r}
# install.packages("Metrics")
# library(Metrics)

predictedvalue <- predict(tree_0.020,data)
mse(data$TOTAL13,predictedvalue)

#double check the function mse() is working
mean((data$TOTAL13-predictedvalue)^2)


```

As MSE of regression model is lower than MSE of tree model. Therefore, we should choose regression model over tree model to minimize the errors. 

#Reference: 
textbook, PennState Eberly College of Science (https://onlinecourses.science.psu.edu/stat501), University of Virginia Library (http://data.library.virginia.edu/understanding-q-q-plots/)

#Question 2
Find two data sets from the UCI data repository or R datasets. Conduct a detailed regression analysis for both datasets using both regression model and the tree model (for regression), e.g., for regression model, you may want to conduct model selection, model comparison, testing of the significance of the regression parameters, evaluation of the R-squared and significance of the model. Also comment on the application of your model on the context of the dataset you have selected.

## Car Dataset
The first dataset we chose for modeling is the cu.summary dataset that contains automobile data taken from the April, 1990 issue of "Consumer Reports". The dataset contains 117 observations and 5 features. We are trying to predict the price of the car. A table of feature descriptions is provided below.

| Feature Name | Description                                                                                 |
|--------------|---------------------------------------------------------------------------------------------|
| Price        | a numeric vector giving the list price in US dollars of a standard model                    |
| Country      | Country of origin                                                                           |
| Reliability  | an ordered factor with levels 'Much worse' < 'worse' < 'average' < 'better' < 'Much better' |
| Mileage      | fuel consumption miles per US gallon                                                        |
| Type         | a factor with levels Compact Large Medium Small Sporty Van                                  |

### Linear Regression
We fit a linear regression model with all of the features to predict the price of the car. 
Looking at the summary, we see that this linear regression model explains 83.7% of the 
variability in the data. 
```{r}
lm.car <- lm(Price~., data=df.car)
summary(lm.car)
```

We use the stepAIC function to identify and discard the insignificant features in our model. 
Looking at the summary, we see that none of the features have been discarded, showing that 
all of the features in our initial linear regression are significant. Also, let us note that 
our initial model is the optimal linear regression model given the data. 

Looking at the summary, we see that the features which increase the price are: the size of the vehicle (medium or large), 
a better reliability, the country it was made in (Japan and Sweden), and the type of vehicle (sporty or van). Thus, three out of the four
factors positively affect the price. The coefficients of the other features were all negative. 

```{r}
lm.car <- stepAIC(lm.car, direction="both")
summary(lm.car)
```

We now analyze our final linear regression model by use of the residual vs fitted and QQ-plots. The residual vs fitted plot 
shows randomly distributed points, which means that the regression assumptions have not been violated. The QQ-plot shows 
points clustered along the line indicating that the dependent variables can plausibly be normally distributed. 

```{r}
par(mar = rep(2, 4)) # Change the margins
plot(lm.car, which=c(1))
plot(lm.car, which=c(2))
```

This model can be used to study which factors have the biggest impact on increasing the price of a vehicle. 
Understanding the extent in which certain features inflate the price of a vehicle can help manufacturers decide which
areas they should focus on when attempting to decrease their prices.


### Decision Tree
A model of the decision tree fitted to our dataset is produced below. The decision tree only incorporates 
two out of the four features; the tree incorporates the type and country of origin of each vehicle while 
it discards the information pertaining to the reliability and mileage of the vehicle. In contrast, 
the linear regression model incorporated all four of the given features and showed that including three of the 
features would always positively affect the price. This discrepancy between models suggests that each model 
may be able to better explain certain characteristics of the data. 

```{r}
library("rpart.plot")

tr.car <- rpart(Price~., data=df.car)
prp(tr.car, varlen=3)
```

## Males Dataset
The second dataset we chose for modeling is the Males dataset that recorded data about the wages and education of young males. The dataset contained 4360 observations and 12 features.  We are trying to predict the wage of the person. A table of feature descriptions is provided below.

| Feature Name | Description                        |
|--------------|------------------------------------|
| nr           | unique identifier                  |
| year         | year data was collected            |
| school       | years of schooling                 |
| exper        | years of experience                |
| union        | wage set by collective bargaining? |
| ethn         | ethnicity                          |
| married      | are they married?                  |
| health       | health problems?                   |
| wage         | log of hourly wage                 |
| industry     | work industry                      |
| occupation   | job occupation                     |
| residence    | area of residence                  |
### Linear Regression
We first fit an initial linear regression model with all of the features to predict the wage of the individual. From looking at the summary, the initial linear regression model is able to explain 28.54% of the variability of the wage. 

```{r}
lm.males <- lm(wage~., data=df.males)
```
Next we used some feature selection using stepwise regresssion to fit the best model. The final model produced from the stepwise regression found all but the health feature to be signficant. By analyzing the difference in the models, we can see that the R-squared was only reduced to 28.53%. The final found all included features to be significant at a 0.05 level. When looking at the coeffecients, that years of schooling, year of experience, union, ethnicity, and being married all had a positive impact on wages per unit increase while keeping all others constant. Other coeffecients were a mix of positive and negative values depending on the specific categorical value taken. 

```{r}
lm.males <- stepAIC(lm.males, direction="both")
summary(lm.males)
```
After obtaining this final model, we need to analyze the residual vs fitted plot and the QQ-plot to see if any of the linear regression assumptions are violated. From looking at the residual vs fitted plot, the points seem to be randomly distrubted and there are no trends as fitted values increase, so there does not seem to be any violations. When looking at the QQ-plot, there seems to be deviation from normality assumption. 

```{r}
plot(lm.males, which=c(1))
plot(lm.males, which=c(2))
```
The applications of this model could be used to study what types of features are correlated most with a high wage. This could be important in helping to decide policy of whether more education helps to increase wage and to study if there are discrepencies among different socioeconomic factors. 

### Decision Tree
A decision tree was fit to the data and the output of the model is seen below. Compared to the regression model, the decision tree only incorporated the use of 5 out of 11 features. These features are industry, years of schooling, year the data was collected, married, and years of work experience. From the analyzing the rules the decision tree came up with, the observations with the highest wage are those who have 12+ years of school and are not a part of the agriculture, construction, entertainment, personal services, professional services, and transportation. This type can be used to see if there are any underlying patterns in the data that a linear regression might not be able to ascertain and can be used to predict wages of young professional males.
```{r}
tr.males <- rpart(wage~.,data=df.males)
prp(tr.males, nn.cex=1)
```

# Question 5 
Build a decision tree model based on the following dataset. Don't use R. Use your pen and paper, and show the process.

```{r}
df.q5 <- data.frame(x1=c(0.22,0.58,0.57,0.41,0.60,0.12,0.25,0.32), 
                    x2=c(0.38,0.32,0.28,0.43,0.29,0.32,0.32,0.38), 
                    y=factor(c("No","Yes","Yes","Yes","No","Yes","Yes","No")))
 
```

We chose an arbitrary rule of splitting on the 4 quantiles. The first split is on x2 at the 3rd quantile, which is $x2 \leq 0.38$. The left node will contain data points (1,2,3,5,6,7,8) and the right node will contain data point (4).

\[e_{root} = -\frac{5}{8} log_{2}\frac{5}{8} - \frac{3}{8} log_{2}\frac{3}{8} = 0.9544\]
\[e_{x2 \leq 0.38} =  -\frac{4}{7} log_{2}\frac{4}{7} - \frac{3}{7} log_{2}\frac{3}{7} = 0.9852\]
\[e_{x2 \gt 0.38}  -\frac{1}{1} log_{2}\frac{1}{1} - \frac{0}{1} log_{2}\frac{0}{1} = 0\]
\[IG = e_{root} - \frac{7}{8}*e_{x2 \leq 0.38}-\frac{1}{8}*e_{x2 \gt 0.38} = 0.0924\]

The second split was done on left node containing data points (1,2,3,5,6,7,8) on x2 at the 3rd quantile, which is $x2 \leq 0.35$. The left node will contain data points (2,3,5,6,7) and the right node will contain data points (1,8).

\[e_{root} = -\frac{4}{7} log_{2}\frac{4}{7} - \frac{3}{7} log_{2}\frac{3}{7} = 0.9852\]
\[e_{x2 \leq 0.35} =  -\frac{4}{5} log_{2}\frac{4}{5} - \frac{1}{5} log_{2}\frac{1}{5} = 0.7219\]
\[e_{x2 \gt 0.35}  -\frac{0}{2} log_{2}\frac{0}{2} - \frac{2}{2} log_{2}\frac{2}{2} = 0\]
\[IG = e_{root} - \frac{5}{7}*e_{x2 \leq 0.35}-\frac{2}{7}*e_{x2 \gt 0.35} = 0.4696\]

The third split was on the left node containin data points (2,3,5,6,7) on x1 at the 3rd quantile, which is $x1 \leq 0.58$. The left node will contain data points (2,3,6,7) and the right node will contain data point (5). 

\[e_{root} = -\frac{4}{5} log_{2}\frac{4}{5} - \frac{1}{5} log_{2}\frac{1}{5} = 0.7219\]
\[e_{x1 \leq 0.58} =  -\frac{4}{4} log_{2}\frac{4}{4} - \frac{0}{4} log_{2}\frac{0}{4} = 0\]
\[e_{x1 \gt 0.58}  -\frac{0}{1} log_{2}\frac{0}{1} - \frac{1}{1} log_{2}\frac{1}{1} = 0\]
\[IG = e_{root} - \frac{4}{5}*e_{x1 \leq 0.58}-\frac{1}{5}*e_{x1 \gt 0.58} = 0.7219\]


# Question 6
Write your own R script to implement the least squares estimation of a regression model. Compare the output from your script with the output from lm().

We arbitrarily choose the number of input variables as p = 3, and define the following function. 

```{r}
#least_squares: Returns the least squares estimation of a regression model
least_squares <- function(x1, x2, x3, y){ 
  
  # Creating the X and Y matrices
  x_matrix = as.matrix(cbind(rep(1, 20), x1, x2, x3))
  y_matrix = as.matrix(y)
  
  # Computing the beta estimator
  beta_estimator = solve( t(x_matrix) %*% x_matrix ) %*% t(x_matrix) %*% y_matrix
  
  return(beta_estimator)
} 
```

We can now compare our function against the output from lm()

```{r}
# Choose random x1, x2, x3 inputs
x1 = runif(1:20)
x2 = runif(1:20)
x3 = runif(1:20)

# Create an arbitrary y 
y = 4 + 2*x1 + 1*x2 + 8*x3

# Call our function: least_squares 
least_squares(x1, x2, x3, y)

# Calling the R function for linear regression: lm 
lm(y~., data = data.frame(x1, x2, x3, y))

```
