---
title: "INDE498_HW1"
output: html_document
---

```{r setup, include=FALSE}
df.forbes <- read.csv("forbes.csv")
df.males <- read.csv("Males.csv")
```

#### Question 1 ####
Repeat the analysis shown in the R lab of this chapter, but use TOTAL13 as the outcome variable. Please build both the regression model and the decision tree model (for regression). Identify the final models you would select, evaluate the models, and compare the regression model with the tree model.


#### Question 2 ####
Find two data sets from the UCI data repository or R datasets. Conduct a detailed regression analysis for both datasets using both regression model and the tree model (for regression), e.g., for regression model, you may want to conduct model selection, model comparison, testing of the significance of the regression parameters, evaluation of the R-squared and significance of the model. Also comment on the application of your model on the context of the dataset you have selected.

#### Question 5 #### 
Build a decision tree model based on the following dataset. Don't use R. Use your pen and paper, and show the process.

```{r}
df.q5 <- data.frame(x1=c(0.22,0.58,0.57,0.41,0.60,0.12,0.25,0.32), 
                    x2=c(0.38,0.32,0.28,0.43,0.29,0.32,0.32,0.38), 
                    y=factor(c("No","Yes","Yes","Yes","No","Yes","Yes","No")))
 
```

We chose an arbitrary rule of splitting on the 4 quantiles. The first split is on x2 at the 3rd quantile, which is $x2 \leq 0.38$. The left node will contain data points (1,2,3,5,6,7,8) and the right node will contain data point (4).

\[e_{root} = -\frac{5}{8} log_{2}\frac{5}{8} - \frac{3}{8} log_{2}\frac{3}{8} = 0.9544\]
\[e_{x2 \leq 0.38} =  -\frac{4}{7} log_{2}\frac{4}{7} - \frac{3}{7} log_{2}\frac{3}{7} = 0.9852\]
\[e_{x2 \gt 0.38}  -\frac{1}{1} log_{2}\frac{1}{1} - \frac{0}{1} log_{2}\frac{0}{1} = 0\]
\[IG = e_{root} - \frac{7}{8}*e_{x2 \leq 0.38}-\frac{1}{8}*e_{x2 \gt 0.38} = 0.0924\]

The second split was done on left node containing data points (1,2,3,5,6,7,8) on x2 at the 3rd quantile, which is $x2 \leq 0.35$. The left node will contain data points (2,3,5,6,7) and the right node will contain data points (1,8).

\[e_{root} = -\frac{4}{7} log_{2}\frac{4}{7} - \frac{3}{7} log_{2}\frac{3}{7} = 0.9852\]
\[e_{x2 \leq 0.35} =  -\frac{4}{5} log_{2}\frac{4}{5} - \frac{1}{5} log_{2}\frac{1}{5} = 0.7219\]
\[e_{x2 \gt 0.35}  -\frac{0}{2} log_{2}\frac{0}{2} - \frac{2}{2} log_{2}\frac{2}{2} = 0\]
\[IG = e_{root} - \frac{5}{7}*e_{x2 \leq 0.35}-\frac{2}{7}*e_{x2 \gt 0.35} = 0.4696\]

The third split was on the left node containin data points (2,3,5,6,7) on x1 at the 3rd quantile, which is $x1 \leq 0.58$. The left node will contain data points (2,3,6,7) and the right node will contain data point (5). 

\[e_{root} = -\frac{4}{5} log_{2}\frac{4}{5} - \frac{1}{5} log_{2}\frac{1}{5} = 0.7219\]
\[e_{x1 \leq 0.58} =  -\frac{4}{4} log_{2}\frac{4}{4} - \frac{0}{4} log_{2}\frac{0}{4} = 0\]
\[e_{x1 \gt 0.58}  -\frac{0}{1} log_{2}\frac{0}{1} - \frac{1}{1} log_{2}\frac{1}{1} = 0\]
\[IG = e_{root} - \frac{4}{5}*e_{x1 \leq 0.58}-\frac{1}{5}*e_{x1 \gt 0.58} = 0.7219\]


#### Question 6 ####
Write your own R script to implement the least squares estimation of a regression model. Compare the output from your script with the output from lm().

We arbitrarily choose the number of input variables as p = 3, and define the following function. 

```{r}
#least_squares: Returns the least squares estimation of a regression model
least_squares <- function(x1, x2, x3, y){ 
  
  # Creating the X and Y matrices
  x_matrix = as.matrix(cbind(rep(1, 20), x1, x2, x3))
  y_matrix = as.matrix(y)
  
  # Computing the beta estimator
  beta_estimator = solve( t(x_matrix) %*% x_matrix ) %*% t(x_matrix) %*% y_matrix
  
  return(beta_estimator)
} 
```

We can now compare our function against the output from lm()

```{r}
# Choose random x1, x2, x3 inputs
x1 = runif(1:20)
x2 = runif(1:20)
x3 = runif(1:20)

# Create an arbitrary y 
y = 4 + 2*x1 + 1*x2 + 8*x3

# Call our function: least_squares 
least_squares(x1, x2, x3, y)

# Calling the R function for linear regression: lm 
lm(y~., data = data.frame(x1, x2, x3, y))

```
